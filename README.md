# Open-Interpreter Inferencing Guide
Complete guide to infer the open-interpreter with ChatGPT as well as llama2 with a nice CLI UI.

Welcome to the Open-Interpreter Inference project. This guide will walk you through the process of setting up and running the model within a neat Command Line Interface (CLI).

> **Note:** This setup assumes you are operating within a virtual environment. Ensure you're referencing the correct directory paths inside this environment when following the steps below.

## Installation

1. **Installing Necessary Packages**:
   - Activate your virtual environment.
   - Install `llama-cpp-python` using the command:
     ```
     pip install llama-cpp-python
     ```
   - Next, install `open-interpreter`:
     ```
     pip install open-interpreter
     ```

## Configuration

2. **Editing the Python File**:
   - Open the provided Python file.
   - Modify as needed, for instance, adjusting the model or toggling `interpreter.auto_run` to `true` or `false`.
   - Save the file after your edits.

3. **Editing the Bash Scripting File**:
   - Locate the `Bash_UI.txt` file.
   - Open it in Notepad or your preferred text editor.
   - Update with the required directory paths, keeping the virtual environment in mind.

## Execution

4. **Saving and Running the Batch File**:
   - Save your changes to the file with a `.bat` extension.
   - Launch it either by double-clicking or executing it via command prompt.

## Interaction

5. **Engaging with Open-Interpreter Inference via CLI UI**:
   - Post-launch of the `.bat` file, the interface should be up and running.
   - Engage with the Command Line Interface (CLI) for a streamlined user experience.

---

For any challenges, please refer back to this guide. Your feedback and contributions are always welcome!
